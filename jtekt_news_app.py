import streamlit as st
import pandas as pd
import datetime
import traceback
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';
            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
    except Exception as e:
        print(f"âš ï¸ éè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºæœ¬ä½“
def scrape_articles(year, start_date, end_date):
    print("ğŸš€ scrape_articles é–‹å§‹")
    driver = generate_driver()
    data = []
    page_num = 1
    status = st.empty()

    while True:
        url = get_page_url(year, page_num)
        print(f"ğŸŒ€ ã‚¢ã‚¯ã‚»ã‚¹URL: {url}")
        driver.get(url)

        try:
            hide_cookie_popup(driver)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, '//li[@class="article"]'))
            )
            time.sleep(1)
        except:
            print(f"âœ… ãƒšãƒ¼ã‚¸{page_num}ãŒå­˜åœ¨ã—ãªã„ãŸã‚çµ‚äº†")
            break

        articles = driver.find_elements(By.XPATH, '//li[@class="article"]')
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸{page_num} ã®è¨˜äº‹æ•°: {len(articles)}")

        def scrape_articles(year):
    print("ğŸš€ scrape_articles é–‹å§‹")
    driver = generate_driver()
    data = []
    page_num = 1

    # ğŸ¯ å›ºå®šã®æœŸé–“æŒ‡å®šï¼š2024å¹´5æœˆã®ã¿
    start_date = pd.to_datetime("2024-05-31")
    end_date = pd.to_datetime("2024-05-01")

    while True:
        url = get_page_url(year, page_num)
        print(f"ğŸŒ€ ã‚¢ã‚¯ã‚»ã‚¹URL: {url}")
        driver.get(url)

        try:
            hide_cookie_popup(driver)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, '//li[@class="article"]'))
            )
            time.sleep(1)
        except:
            print(f"âœ… ãƒšãƒ¼ã‚¸{page_num}ãŒå­˜åœ¨ã—ãªã„ãŸã‚çµ‚äº†")
            break

        articles = driver.find_elements(By.XPATH, '//li[@class="article"]')
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸{page_num} ã®è¨˜äº‹æ•°: {len(articles)}")

        for article in articles:
            try:
                link = article.find_element(By.XPATH, './/a').get_attribute('href')
                title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
                date = article.find_element(By.XPATH, './/time').text

                date_obj = pd.to_datetime(date, format="%Y.%m.%d", errors="coerce")
                print(f"ğŸ—“ï¸ æŠ½å‡ºå€™è£œ: {date} â†’ {date_obj}")

                if pd.isna(date_obj):
                    continue

                # ğŸ¯ 2024å¹´5æœˆã«çµã‚Šè¾¼ã¿
                if date_obj > start_date or date_obj < end_date:
                    print(f"â© {date} ã¯5æœˆä»¥å¤– â†’ ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                if any(skip in link for skip in ["/ir/", "/engineering-journal/", "irmovie.jp"]):
                    print(f"ğŸ“„ ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ãƒªãƒ³ã‚¯: {link}")
                    data.append({"æ—¥ä»˜": date, "è¦‹å‡ºã—": title, "æœ¬æ–‡": "ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡", "ãƒªãƒ³ã‚¯": link})
                    continue

                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(link)

                WebDriverWait(driver, 10).until(
                    EC.visibility_of_element_located((By.CLASS_NAME, "detail-content"))
                )
                soup = BeautifulSoup(driver.page_source, "html.parser")
                content_div = soup.select_one("div.detail-content")

                if content_div:
                    body_text = "\n".join(
                        tag.get_text(strip=True) for tag in content_div.find_all(["h2", "p"])
                    )
                else:
                    body_text = "æœ¬æ–‡æŠ½å‡ºä¸å¯"

                data.append({
                    "æ—¥ä»˜": date,
                    "è¦‹å‡ºã—": title,
                    "æœ¬æ–‡": body_text.strip(),
                    "ãƒªãƒ³ã‚¯": link
                })

                print(f"âœ… æŠ½å‡ºæˆåŠŸ: {title}")

                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                time.sleep(1)

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                traceback.print_exc()
                try:
                    driver.switch_to.window(driver.window_handles[0])
                except:
                    pass
                continue

        page_num += 1

    driver.quit()
    return pd.DataFrame(data)

#Streamlit UI------
st.title("JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºï¼ˆ2024å¹´5æœˆé™å®šï¼‰")

if st.button("âœ… 2024å¹´5æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"):
    with st.spinner("è¨˜äº‹ã‚’æŠ½å‡ºä¸­ã§ã™..."):
        df = scrape_articles(2024)
        if df.empty:
            st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.success(f"{len(df)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
            st.dataframe(df)
            st.download_button(
                label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=df.to_csv(index=False),
                file_name="jtekt_news_2024_05.csv",
                mime="text/csv"
            )
