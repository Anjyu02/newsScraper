import streamlit as st
import pandas as pd
import datetime
import traceback
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';
            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
    except Exception as e:
        print(f"âš ï¸ éè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºæœ¬ä½“
def scrape_articles(year, start_date, end_date):
    print("ğŸš€ scrape_articles é–‹å§‹")
    driver = generate_driver()
    data = []
    page_num = 1
    status = st.empty()

    while True:
        url = get_page_url(year, page_num)
        print(f"ğŸŒ€ ã‚¢ã‚¯ã‚»ã‚¹URL: {url}")
        driver.get(url)

        try:
            hide_cookie_popup(driver)
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, '//li[@class="article"]'))
            )
            time.sleep(1)
        except:
            print(f"âœ… ãƒšãƒ¼ã‚¸{page_num}ãŒå­˜åœ¨ã—ãªã„ãŸã‚çµ‚äº†")
            break

        articles = driver.find_elements(By.XPATH, '//li[@class="article"]')
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸{page_num} ã®è¨˜äº‹æ•°: {len(articles)}")

        for article in articles:
            try:
                link = article.find_element(By.XPATH, './/a').get_attribute('href')
                title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
                date = article.find_element(By.XPATH, './/time').text

                date_obj = pd.to_datetime(date, format="%Y.%m.%d", errors="coerce")
                print(f"ğŸ—“ï¸ æŠ½å‡ºå€™è£œ: {date} â†’ {date_obj}")

                if pd.isna(date_obj):
                    print(f"âŒ æ—¥ä»˜å¤‰æ›å¤±æ•—: {date}")
                    continue

                # âœ… æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆæ–°ã—ã„â†’å¤ã„ï¼‰
                if date_obj > pd.to_datetime(start_date):
                    print(f"â© {date} ã¯é–‹å§‹æ—¥ {start_date} ã‚ˆã‚Šæ–°ã—ã„ â†’ ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                if date_obj < pd.to_datetime(end_date):
                    print(f"ğŸ›‘ {date} ã¯çµ‚äº†æ—¥ {end_date} ã‚ˆã‚Šå‰ â†’ é¡è¡Œçµ‚äº†")
                    driver.quit()
                    return pd.DataFrame(data)

                # âœ… ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã®ãƒªãƒ³ã‚¯åˆ¤å®š
                if any(skip in link for skip in ["/ir/", "/engineering-journal/", "irmovie.jp"]):
                    print(f"ğŸ“„ ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ãƒªãƒ³ã‚¯: {link}")
                    data.append({"æ—¥ä»˜": date, "è¦‹å‡ºã—": title, "æœ¬æ–‡": "ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡", "ãƒªãƒ³ã‚¯": link})
                    continue

                # âœ… æœ¬æ–‡æŠ½å‡º
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(link)

                WebDriverWait(driver, 10).until(
                    EC.visibility_of_element_located((By.CLASS_NAME, "detail-content"))
                )
                soup = BeautifulSoup(driver.page_source, "html.parser")
                content_div = soup.select_one("div.detail-content")

                if content_div:
                    body_text = "\n".join(
                        tag.get_text(strip=True) for tag in content_div.find_all(["h2", "p"])
                    )
                else:
                    body_text = "æœ¬æ–‡æŠ½å‡ºä¸å¯"

                data.append({
                    "æ—¥ä»˜": date,
                    "è¦‹å‡ºã—": title,
                    "æœ¬æ–‡": body_text.strip(),
                    "ãƒªãƒ³ã‚¯": link
                })

                print(f"âœ… æŠ½å‡ºæˆåŠŸ: {title}")

                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                time.sleep(1)

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆè¨˜äº‹ã”ã¨ã®å‡¦ç†ï¼‰: {e}")
                traceback.print_exc()
                try:
                    driver.switch_to.window(driver.window_handles[0])
                except:
                    pass
                continue

        page_num += 1

    # âœ… whileæ–‡ã‚’æŠœã‘ãŸå¾Œã€é–¢æ•°æœ¬ä½“ã®æœ«å°¾ã§æƒãˆã‚‹
    print("ğŸ“¦ åé›†ä»¶æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‰ï¼‰:", len(data))
    driver.quit()
    return pd.DataFrame(data)

# ===============================
# âœ… Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“
# ===============================
st.title("JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºã‚¢ãƒ—ãƒª")

today = datetime.date.today()
start_date = st.date_input("é–‹å§‹æ—¥ï¼ˆä»Šæ—¥ã«è¿‘ã„æ—¥ï¼‰", today)
default_end_date = datetime.date(start_date.year, 1, 1)
end_date = st.date_input("çµ‚äº†æ—¥ï¼ˆã©ã“ã¾ã§é¡ã‚‹ã‹ï¼‰", default_end_date)

st.caption("â€» JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã¯æ–°ã—ã„é †ã«ä¸¦ã‚“ã§ã„ã‚‹ãŸã‚ã€é–‹å§‹æ—¥ã¯ä»Šæ—¥ã«è¿‘ã„æ—¥ã€çµ‚äº†æ—¥ã¯é¡ã‚ŠãŸã„éå»ã®æ—¥ã«ã—ã¦ãã ã•ã„ã€‚")

if end_date > start_date:
    st.error("âš ï¸ çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥å‰ã®æ—¥ä»˜ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
else:
    if st.button("âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡ºã™ã‚‹"):
        with st.spinner("è¨˜äº‹ã‚’æŠ½å‡ºä¸­ã§ã™..."):
            df = scrape_articles(start_date.year, start_date, end_date)

            if df.empty:
                st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                df["æ—¥ä»˜_dt"] = pd.to_datetime(df["æ—¥ä»˜"], format="%Y.%m.%d", errors="coerce")
                df_filtered = df[
                    (df["æ—¥ä»˜_dt"] <= pd.to_datetime(start_date)) &
                    (df["æ—¥ä»˜_dt"] >= pd.to_datetime(end_date))
                ]

                if df_filtered.empty:
                    st.warning("æŒ‡å®šã—ãŸæœŸé–“ã«è©²å½“ã™ã‚‹è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    st.success(f"{len(df_filtered)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
                    st.dataframe(df_filtered.drop(columns=["æ—¥ä»˜_dt"]))
                    st.download_button(
                        label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=df_filtered.drop(columns=["æ—¥ä»˜_dt"]).to_csv(index=False),
                        file_name=f"jtekt_news_{start_date}_{end_date}.csv",
                        mime="text/csv"
                    )
if st.button("âœ… ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"):
    df = scrape_articles(2025, datetime.date(2025, 7, 5), datetime.date(2025, 1, 1))
    st.write(df)
