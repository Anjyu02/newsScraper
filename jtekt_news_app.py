import streamlit as st
import pandas as pd
import time
import traceback
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from datetime import datetime

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';
            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
    except Exception as e:
        print(f"âš ï¸ éè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def get_yearly_date_ranges(start_date, end_date):
    """
    ä»»æ„ã®æœŸé–“ã‚’å¹´ã”ã¨ã«åˆ†å‰²ã—ã€
    JTEKTã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸æ§‹é€ ï¼ˆå¹´åˆ¥URLï¼‰ã«å¯¾å¿œã™ã‚‹å¹´å˜ä½ã®æ—¥ä»˜ç¯„å›²ã‚’è¿”ã™ã€‚
    """
    if start_date < end_date:
        # æ–°ã—ã„æ—¥ â†’ å¤ã„æ—¥ ã¨ã„ã†é †ã«æƒãˆã‚‹
        start_date, end_date = end_date, start_date

    year_ranges = {}
    for year in range(end_date.year, start_date.year + 1):
        y_start = min(start_date, pd.to_datetime(f"{year}-12-31"))
        y_end = max(end_date, pd.to_datetime(f"{year}-01-01"))

        if y_start >= y_end:
            year_ranges[year] = (y_start, y_end)

    return dict(sorted(year_ranges.items(), reverse=True))

# ===============================
# âœ… æœ¬æ–‡æŠ½å‡ºãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ===============================
def extract_content_text(content_div):
    texts = []

    # 1. <p>ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ”¹è¡Œå«ã‚€ï¼‰
    for p_tag in content_div.find_all("p"):
        paragraph = p_tag.get_text(" ", strip=True)
        if paragraph:
            texts.append(paragraph)

    # 2. <table>ã‚¿ã‚°ã®å‡¦ç†ï¼ˆè¡Œå˜ä½ã« "ï½œ" åŒºåˆ‡ã‚Šã§æŠ½å‡ºï¼‰
    for table in content_div.find_all("table"):
        for row in table.find_all("tr"):
            cells = row.find_all(["th", "td"])
            row_text = "ï½œ".join(cell.get_text(strip=True) for cell in cells)
            texts.append(row_text)

    return "\n".join(texts)

# ===============================
# âœ… æœ¬ä½“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
# ===============================
def scrape_articles(year, start_date, end_date):
    print(f"ğŸš€ scrape_articles é–‹å§‹: {year}å¹´ï¼ˆ{start_date.date()}ã€œ{end_date.date()}ï¼‰")
    driver = generate_driver()
    data = []
    page_num = 1

    # âœ… Streamlit è¡¨ç¤ºç”¨ã‚¨ãƒªã‚¢
    status = st.empty()

    while True:
        url = get_page_url(year, page_num)
        print(f"ğŸŒ€ ã‚¢ã‚¯ã‚»ã‚¹URL: {url}")
        driver.get(url)

        try:
            hide_cookie_popup(driver)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, '//li[@class="article"]'))
            )
            time.sleep(1)
        except:
            print(f"âœ… ãƒšãƒ¼ã‚¸{page_num}ãŒå­˜åœ¨ã—ãªã„ãŸã‚çµ‚äº†")
            break

        articles = driver.find_elements(By.XPATH, '//li[@class="article"]')
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸{page_num} ã®è¨˜äº‹æ•°: {len(articles)}")

        for article in articles:
            try:
                link = article.find_element(By.XPATH, './/a').get_attribute('href')
                title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
                date = article.find_element(By.XPATH, './/time').text
                date_obj = pd.to_datetime(date, format="%Y.%m.%d", errors="coerce")

                print(f"ğŸ—“ï¸ æŠ½å‡ºå€™è£œ: {date} â†’ {date_obj}")
                if pd.isna(date_obj):
                    continue

                status.text(f"ğŸ“… ç¾åœ¨å‡¦ç†ä¸­ã®æ—¥ä»˜: {date}")

                # âœ… æœŸé–“åˆ¤å®šï¼ˆæ–°ã—ã„é †ã«ä¸¦ã¶æ§‹é€ å‰æï¼‰
                if date_obj < end_date:
                    print(f"ğŸ›‘ {date} ã¯ç¯„å›²ã‚ˆã‚Šå¤ã„ãŸã‚æ‰“ã¡åˆ‡ã‚Š")
                    driver.quit()
                    return pd.DataFrame(data)
                elif date_obj > start_date:
                    print(f"â© {date} ã¯ç¯„å›²ã‚ˆã‚Šæ–°ã—ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # âœ… ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ãƒªãƒ³ã‚¯å‡¦ç†
                skip_reason = None
                if "/ir/" in link:
                    skip_reason = "IRãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—"
                elif "/engineering-journal/" in link:
                    skip_reason = "Engineering Journalãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—"
                elif "irmovie.jp" in link:
                    skip_reason = "å¤–éƒ¨ã‚µã‚¤ãƒˆã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—"

                if skip_reason:
                    print(f"ğŸ“„ {skip_reason} â†’ {link}")
                    data.append({
                        "æ—¥ä»˜": date,
                        "è¦‹å‡ºã—": title,
                        "æœ¬æ–‡": skip_reason,
                        "ãƒªãƒ³ã‚¯": link
                    })
                    continue

                # âœ… æœ¬æ–‡æŠ½å‡º
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(link)

                WebDriverWait(driver, 10).until(
                    EC.visibility_of_element_located((By.CLASS_NAME, "detail-content"))
                )
                soup = BeautifulSoup(driver.page_source, "html.parser")
                content_div = soup.select_one("div.detail-content")

                # âœ… ãƒ†ãƒ¼ãƒ–ãƒ«å«ã‚€æŠ½å‡ºå‡¦ç†
                if content_div:
                    body_text = extract_content_text(content_div)
                else:
                    body_text = "æœ¬æ–‡æŠ½å‡ºä¸å¯"

                data.append({
                    "æ—¥ä»˜": date,
                    "è¦‹å‡ºã—": title,
                    "æœ¬æ–‡": body_text.strip(),
                    "ãƒªãƒ³ã‚¯": link
                })

                print(f"âœ… æŠ½å‡ºæˆåŠŸ: {title}")

                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                time.sleep(1)

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                traceback.print_exc()
                try:
                    driver.switch_to.window(driver.window_handles[0])
                except:
                    pass
                continue

        page_num += 1

    driver.quit()
    return pd.DataFrame(data)
    
# ===============================
# âœ… Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“ï¼ˆä»»æ„æœŸé–“å¯¾å¿œï¼‰
# ===============================

st.title("JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºï¼ˆä»»æ„æœŸé–“ï¼‰")

# âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã™ã‚‹æœŸé–“ï¼ˆé–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ï¼‰
start_date = st.date_input("é–‹å§‹æ—¥ï¼ˆæ–°ã—ã„æ—¥ä»˜ï¼‰", datetime.today())
end_date = st.date_input("çµ‚äº†æ—¥ï¼ˆå¤ã„æ—¥ä»˜ï¼‰", datetime.today())

if start_date > end_date:
    if st.button("âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡ºã™ã‚‹"):
        with st.spinner("è¨˜äº‹ã‚’æŠ½å‡ºä¸­ã§ã™..."):

            # 1. å¹´ã”ã¨ã®å‡¦ç†ç¯„å›²ã‚’å–å¾—
            year_ranges = get_yearly_date_ranges(
                pd.to_datetime(start_date), pd.to_datetime(end_date)
            )

            all_data = []

            # 2. å„å¹´ã”ã¨ã«è¨˜äº‹ã‚’å–å¾—
            for year, (y_start, y_end) in year_ranges.items():
                st.write(f"ğŸ“… {year}å¹´: {y_start.date()} ã€œ {y_end.date()} ã‚’å‡¦ç†ä¸­...")
                df_year = scrape_articles(year, y_start, y_end)
                all_data.append(df_year)

            # 3. ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨è¡¨ç¤º
            if all_data:
                df_all = pd.concat(all_data, ignore_index=True)
                df_all["æ—¥ä»˜_dt"] = pd.to_datetime(df_all["æ—¥ä»˜"], format="%Y.%m.%d", errors="coerce")
                df_all = df_all.sort_values("æ—¥ä»˜_dt", ascending=False).drop(columns=["æ—¥ä»˜_dt"])

                st.success(f"âœ… {len(df_all)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
                st.dataframe(df_all)

                st.download_button(
                    label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=df_all.to_csv(index=False),
                    file_name="jtekt_news_selected_period.csv",
                    mime="text/csv"
                )
            else:
                st.warning("âš ï¸ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    st.error("âš ï¸ çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ã‚ˆã‚Šéå»ã®æ—¥ä»˜ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
