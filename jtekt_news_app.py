import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

# ğŸ”§ ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°å®šç¾©
def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def get_max_page(driver, year):
    url = get_page_url(year, 1)
    driver.get(url)
    hide_cookie_popup(driver) 
    time.sleep(1)
    page_links = driver.find_elements(By.XPATH, '//ul[@class="pager-box"]/li/a')
    page_numbers = set()
    for a in page_links:
        href = a.get_attribute("href")
        if href and f"news{year}" in href:
            if f"news{year}.html" in href:
                page_numbers.add(1)
            elif f"news{year}_" in href:
                try:
                    num = int(href.split(f"news{year}_")[1].split(".html")[0])
                    page_numbers.add(num)
                except:
                    pass
    return max(page_numbers) if page_numbers else 1

# Cookieãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’éè¡¨ç¤ºã«ã™ã‚‹é–¢æ•°
def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';

            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
        print("âœ… Cookieãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’éè¡¨ç¤ºã«ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âš ï¸ éè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")



WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.XPATH, '//div//p[@class="article-txt"]'))
)
time.sleep(3)

hide_cookie_popup(driver)

# ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ä¸€è¦§ã‹ã‚‰æŠ½å‡º
data = []
articles = driver.find_elements(By.XPATH, '//li[@class="article"]')

for article in articles:
    try:
        # ğŸ”— å…ˆã«ãƒªãƒ³ã‚¯ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æ—¥ä»˜ã‚’å–å¾—
        link = article.find_element(By.XPATH, './/a').get_attribute('href')
        title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
        date = article.find_element(By.XPATH, './/time').text

        print("âœ… å‡¦ç†ä¸­ãƒªãƒ³ã‚¯:", link)

        # ğŸš« æœ¬æ–‡æŠ½å‡ºå¯¾è±¡å¤–ãƒšãƒ¼ã‚¸ï¼ˆä»¥ä¸‹ã«è©²å½“ã™ã‚‹ãƒªãƒ³ã‚¯ã¯æœ¬æ–‡ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        # â‘  IRãƒšãƒ¼ã‚¸ï¼ˆä¾‹ï¼š/ir/ ã‚’å«ã‚€ï¼‰
        if "/ir/" in link:
            print(f"ğŸ“„ IRãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ— â†’ {link}")
            data.append({
                "æ—¥ä»˜": date,
                "è¦‹å‡ºã—": title,
                "æœ¬æ–‡": "IRãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—",
                "ãƒªãƒ³ã‚¯": link
            })
            continue

        # â‘¡ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ãƒšãƒ¼ã‚¸ï¼ˆä¾‹ï¼š/engineering-journal/ ã‚’å«ã‚€ï¼‰
        elif "/engineering-journal/" in link:
            print(f"ğŸ“„ Engineering Journalãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ— â†’ {link}")
            data.append({
                "æ—¥ä»˜": date,
                "è¦‹å‡ºã—": title,
                "æœ¬æ–‡": "Engineering Journalãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—",
                "ãƒªãƒ³ã‚¯": link
            })
            continue

        # â‘¢ å¤–éƒ¨å‹•ç”»ã‚µã‚¤ãƒˆï¼ˆä¾‹ï¼širmovie.jpï¼‰
        elif "irmovie.jp" in link:
            print(f"ğŸ“„ å¤–éƒ¨å‹•ç”»ãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ— â†’ {link}")
            data.append({
                "æ—¥ä»˜": date,
                "è¦‹å‡ºã—": title,
                "æœ¬æ–‡": "å¤–éƒ¨å‹•ç”»ãƒšãƒ¼ã‚¸ã®ãŸã‚æœ¬æ–‡æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—",
                "ãƒªãƒ³ã‚¯": link
            })
            continue

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã¸ã‚¢ã‚¯ã‚»ã‚¹
        driver.execute_script("window.open('');")
        driver.switch_to.window(driver.window_handles[1])
        driver.get(link)

        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script('return document.readyState') == 'complete'
        )
        hide_cookie_popup(driver)

        WebDriverWait(driver, 20).until(
            EC.visibility_of_element_located((By.XPATH, '//div[@class="detail-content"]'))
        )

        # BeautifulSoupã§æœ¬æ–‡æŠ½å‡º
        soup = BeautifulSoup(driver.page_source, "html.parser")
        content_div = soup.select_one("div.detail-content")

        body_text = ""
        for tag in content_div.find_all(["h2", "p"]):
            body_text += tag.get_text(strip=True) + "\n"

        data.append({
            "æ—¥ä»˜": date,
            "è¦‹å‡ºã—": title,
            "æœ¬æ–‡": body_text.strip(),
            "ãƒªãƒ³ã‚¯": link
        })

        # ã‚¿ãƒ–ã‚’é–‰ã˜ã¦å…ƒã«æˆ»ã‚‹
        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        time.sleep(1)

    except Exception as e:
        print("âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ:")
        traceback.print_exc()
        try:
            driver.switch_to.window(driver.window_handles[0])
        except:
            pass
        continue

# DataFrameåŒ–
df = pd.DataFrame(data)

# è¡¨ç¤º
st.success(f"{len(df)} ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
st.dataframe(df)

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³import streamlit as st
import pandas as pd
import time
import traceback
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';
            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
    except Exception as e:
        print(f"âš ï¸ éè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def scrape_articles():
    driver = generate_driver()
    driver.get(get_page_url(2024, 1))
    hide_cookie_popup(driver)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div//p[@class="article-txt"]'))
    )
    time.sleep(2)

    data = []
    articles = driver.find_elements(By.XPATH, '//li[@class="article"]')

    for article in articles:
        try:
            link = article.find_element(By.XPATH, './/a').get_attribute('href')
            title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
            date = article.find_element(By.XPATH, './/time').text

            if any(skip in link for skip in ["/ir/", "/engineering-journal/", "irmovie.jp"]):
                data.append({"æ—¥ä»˜": date, "è¦‹å‡ºã—": title, "æœ¬æ–‡": "ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡", "ãƒªãƒ³ã‚¯": link})
                continue

            driver.execute_script("window.open('');")
            driver.switch_to.window(driver.window_handles[1])
            driver.get(link)
            WebDriverWait(driver, 10).until(lambda d: d.execute_script('return document.readyState') == 'complete')
            hide_cookie_popup(driver)
            WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//div[@class="detail-content"]')))

            soup = BeautifulSoup(driver.page_source, "html.parser")
            content_div = soup.select_one("div.detail-content")
            body_text = "\n".join(tag.get_text(strip=True) for tag in content_div.find_all(["h2", "p"]))

            data.append({"æ—¥ä»˜": date, "è¦‹å‡ºã—": title, "æœ¬æ–‡": body_text.strip(), "ãƒªãƒ³ã‚¯": link})

            driver.close()
            driver.switch_to.window(driver.window_handles[0])
            time.sleep(1)

        except Exception as e:
            traceback.print_exc()
            try:
                driver.switch_to.window(driver.window_handles[0])
            except:
                pass
            continue

    driver.quit()
    return pd.DataFrame(data)

# Streamlit UI
st.title("JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºã‚¢ãƒ—ãƒª")
if st.button("å®Ÿè¡Œã™ã‚‹"):
    df = scrape_articles()
    st.success(f"{len(df)}ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    st.dataframe(df)
    st.download_button("CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", df.to_csv(index=False), "jtekt_news.csv", "text/csv")

st.download_button(
    label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=df.to_csv(index=False),
    file_name="jtekt_news.csv",
    mime="text/csv"
)
