import streamlit as st
import pandas as pd
import time
import traceback
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';
            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
    except Exception as e:
        print(f"âš ï¸ éžè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def scrape_articles():
    driver = generate_driver()
    driver.get(get_page_url(2024, 1))
    hide_cookie_popup(driver)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div//p[@class="article-txt"]'))
    )
    time.sleep(2)

    data = []
    articles = driver.find_elements(By.XPATH, '//li[@class="article"]')

def scrape_articles(year):
    driver = generate_driver()
    driver.get(get_page_url(year, 1))  # â† å¹´åº¦ã‚’æ¸¡ã™ã‚ˆã†ã«ä¿®æ­£
    hide_cookie_popup(driver)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div//p[@class="article-txt"]'))
    )
    time.sleep(2)

    data = []
    articles = driver.find_elements(By.XPATH, '//li[@class="article"]')

def scrape_articles(year):
    driver = generate_driver()
    driver.get(get_page_url(year, 1))
    hide_cookie_popup(driver)
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div//p[@class="article-txt"]'))
    )
    time.sleep(2)

    data = []
    articles = driver.find_elements(By.XPATH, '//li[@class="article"]')

    for article in articles:
        try:
            link = article.find_element(By.XPATH, './/a').get_attribute('href')
            title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
            date = article.find_element(By.XPATH, './/time').text

            if any(skip in link for skip in ["/ir/", "/engineering-journal/", "irmovie.jp"]):
                data.append({"æ—¥ä»˜": date, "è¦‹å‡ºã—": title, "æœ¬æ–‡": "ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡", "ãƒªãƒ³ã‚¯": link})
                continue

            driver.execute_script("window.open('');")
            driver.switch_to.window(driver.window_handles[1])
            driver.get(link)
            WebDriverWait(driver, 10).until(lambda d: d.execute_script('return document.readyState') == 'complete')
            hide_cookie_popup(driver)
            WebDriverWait(driver, 10).until(
                EC.visibility_of_element_located((By.XPATH, '//div[@class="detail-content"]'))
            )

            soup = BeautifulSoup(driver.page_source, "html.parser")
            content_div = soup.select_one("div.detail-content")
            body_text = "\n".join(
                tag.get_text(strip=True) for tag in content_div.find_all(["h2", "p"])
            )

            data.append({
                "æ—¥ä»˜": date,
                "è¦‹å‡ºã—": title,
                "æœ¬æ–‡": body_text.strip(),
                "ãƒªãƒ³ã‚¯": link
            })

            driver.close()
            driver.switch_to.window(driver.window_handles[0])
            time.sleep(1)

        except Exception as e:
            traceback.print_exc()
            try:
                driver.switch_to.window(driver.window_handles[0])
            except:
                pass
            continue

    driver.quit()
    return pd.DataFrame(data)

# ===============================
# âœ… Streamlitã‚¢ãƒ—ãƒªæœ¬ä½“
# ===============================
import datetime

st.title("JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºã‚¢ãƒ—ãƒª")

# ä»Šæ—¥ã®æ—¥ä»˜å–å¾—
today = datetime.date.today()
start_of_year = datetime.date(today.year, 1, 1)

# ðŸ“… UIè¨­å®šï¼šé–‹å§‹æ—¥ã¯ãã®å¹´ã®å¹´å§‹ã€çµ‚äº†æ—¥ã¯ä»Šæ—¥
start_date = st.date_input("é–‹å§‹æ—¥", start_of_year)
end_date = st.date_input("çµ‚äº†æ—¥", today)

# ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼šçµ‚äº†æ—¥ãŒé–‹å§‹æ—¥ã‚ˆã‚Šå‰ã§ãªã„ã‹
if start_date > end_date:
    st.error("âš ï¸ çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã®æ—¥ä»˜ã‚’é¸æŠžã—ã¦ãã ã•ã„ã€‚")
else:
    if st.button("âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡ºã™ã‚‹"):
        with st.spinner("è¨˜äº‹ã‚’æŠ½å‡ºä¸­ã§ã™..."):
            df = scrape_articles(start_date.year)  # â† é–‹å§‹æ—¥ã‚’é–¢æ•°ã«æ¸¡ã™
            if df.empty:
                st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                try:
                    # "YYYY.MM.DD" ã‚’ datetime ã«å¤‰æ›
                    df["æ—¥ä»˜_dt"] = pd.to_datetime(df["æ—¥ä»˜"], format="%Y.%m.%d", errors="coerce")

                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼šé¸æŠžã—ãŸæœŸé–“å†…ã®è¨˜äº‹ã®ã¿æŠ½å‡º
                    df_filtered = df[(df["æ—¥ä»˜_dt"] >= pd.to_datetime(start_date)) &
                                     (df["æ—¥ä»˜_dt"] <= pd.to_datetime(end_date))]

                    if df_filtered.empty:
                        st.warning("æŒ‡å®šã—ãŸæœŸé–“ã«è©²å½“ã™ã‚‹è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.success(f"{len(df_filtered)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
                        st.dataframe(df_filtered.drop(columns=["æ—¥ä»˜_dt"]))
                        st.download_button(
                            label="ðŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=df_filtered.drop(columns=["æ—¥ä»˜_dt"]).to_csv(index=False),
                            file_name=f"jtekt_news_{start_date}_{end_date}.csv",
                            mime="text/csv"
                        )
                except Exception as e:
                    st.error(f"æ—¥ä»˜å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
