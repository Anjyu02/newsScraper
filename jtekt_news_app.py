import streamlit as st
import pandas as pd
import datetime
import traceback
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromiumService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

def generate_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--disable-features=VizDisplayCompositor")
    driver = webdriver.Chrome(
        service=ChromiumService(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=options
    )
    return driver

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

def hide_cookie_popup(driver):
    try:
        driver.execute_script("""
            let banner = document.querySelector('#onetrust-banner-sdk');
            if (banner) banner.style.display = 'none';
            let overlay = document.querySelector('.onetrust-pc-dark-filter');
            if (overlay) overlay.style.display = 'none';
        """)
    except Exception as e:
        print(f"âš ï¸ éè¡¨ç¤ºå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def get_page_url(year, page_num):
    if page_num == 1:
        return f"https://www.jtekt.co.jp/news/news{year}.html"
    else:
        return f"https://www.jtekt.co.jp/news/news{year}_{page_num}.html"

# âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºæœ¬ä½“
def scrape_articles(year, start_date, end_date):
    driver = generate_driver()
    data = []
    page_num = 1
    status = st.empty()  # âœ… ä¸Šæ›¸ãç”¨è¡¨ç¤ºã‚¨ãƒªã‚¢

    while True:
        url = get_page_url(year, page_num)
        driver.get(url)

        try:
            hide_cookie_popup(driver)
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, '//li[@class="article"]'))
            )
            time.sleep(1)
        except:
            print(f"âœ… ãƒšãƒ¼ã‚¸{page_num}ãŒå­˜åœ¨ã—ãªã„ãŸã‚çµ‚äº†")
            break

        articles = driver.find_elements(By.XPATH, '//li[@class="article"]')
        for article in articles:
            try:
                link = article.find_element(By.XPATH, './/a').get_attribute('href')
                title = article.find_element(By.XPATH, './/p[@class="article-txt"]').text
                date = article.find_element(By.XPATH, './/time').text
                date_obj = pd.to_datetime(date, format="%Y.%m.%d", errors="coerce")

                # âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                status.write(f"ğŸ“„ ãƒšãƒ¼ã‚¸{page_num} | ğŸ“… å‡¦ç†ä¸­ã®æ—¥ä»˜: {date}")

                # âœ… ãƒ•ã‚£ãƒ«ã‚¿ï¼šçµ‚äº†æ—¥ã‚ˆã‚Šéå» â†’ çµ‚äº†
                if date_obj < pd.to_datetime(start_date):
                    print(f"ğŸ›‘ {date} ã¯é–‹å§‹æ—¥ {start_date} ã‚ˆã‚Šå‰ â†’ æŠ½å‡ºçµ‚äº†")
                    driver.quit()
                    return pd.DataFrame(data)

                # âœ… ãƒ•ã‚£ãƒ«ã‚¿ï¼šã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡URL
                if any(skip in link for skip in ["/ir/", "/engineering-journal/", "irmovie.jp"]):
                    data.append({"æ—¥ä»˜": date, "è¦‹å‡ºã—": title, "æœ¬æ–‡": "ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡", "ãƒªãƒ³ã‚¯": link})
                    continue

                # âœ… æœ¬æ–‡æŠ½å‡º
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(link)
                WebDriverWait(driver, 10).until(
                    EC.visibility_of_element_located((By.CLASS_NAME, "detail-content"))
                )
                soup = BeautifulSoup(driver.page_source, "html.parser")
                content_div = soup.select_one("div.detail-content")
                body_text = "\n".join(
                    tag.get_text(strip=True) for tag in content_div.find_all(["h2", "p"])
                )

                data.append({
                    "æ—¥ä»˜": date,
                    "è¦‹å‡ºã—": title,
                    "æœ¬æ–‡": body_text.strip(),
                    "ãƒªãƒ³ã‚¯": link
                })

                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                time.sleep(1)

            except Exception as e:
                traceback.print_exc()
                try:
                    driver.switch_to.window(driver.window_handles[0])
                except:
                    pass
                continue

        page_num += 1

    driver.quit()
    return pd.DataFrame(data)

# ===============================
# âœ… Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“
# ===============================
st.title("JTEKTãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ½å‡ºã‚¢ãƒ—ãƒª")

today = datetime.date.today()
start_of_year = datetime.date(today.year, 1, 1)

start_date = st.date_input("é–‹å§‹æ—¥", start_of_year)
end_date = st.date_input("çµ‚äº†æ—¥", today)

if start_date > end_date:
    st.error("âš ï¸ çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã®æ—¥ä»˜ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
else:
    if st.button("âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡ºã™ã‚‹"):
        with st.spinner("è¨˜äº‹ã‚’æŠ½å‡ºä¸­ã§ã™..."):
            df = scrape_articles(start_date.year, start_date, end_date)
            if df.empty:
                st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                df["æ—¥ä»˜_dt"] = pd.to_datetime(df["æ—¥ä»˜"], format="%Y.%m.%d", errors="coerce")
                df_filtered = df[(df["æ—¥ä»˜_dt"] >= pd.to_datetime(start_date)) &
                                 (df["æ—¥ä»˜_dt"] <= pd.to_datetime(end_date))]
                if df_filtered.empty:
                    st.warning("æŒ‡å®šã—ãŸæœŸé–“ã«è©²å½“ã™ã‚‹è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    st.success(f"{len(df_filtered)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
                    st.dataframe(df_filtered.drop(columns=["æ—¥ä»˜_dt"]))
                    st.download_button(
                        label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=df_filtered.drop(columns=["æ—¥ä»˜_dt"]).to_csv(index=False),
                        file_name=f"jtekt_news_{start_date}_{end_date}.csv",
                        mime="text/csv"
                    )
